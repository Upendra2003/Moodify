{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d18aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45213e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('dreaddit-train.csv')\n",
    "df_test=pd.read_csv('dreaddit-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0283182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load English stopwords\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove digits and single characters\n",
    "    text = re.sub(r'\\b\\w\\b|\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text and remove stopwords\n",
    "    words = [word for word in text.split() if word not in english_stopwords]\n",
    "    \n",
    "    # Stem the words\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join the stemmed words back into a single string\n",
    "    processed_text = ' '.join(stemmed_words)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply the preprocess_text function to the \"text\" column of your DataFrame\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "895a530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "max_features = 10000  # Maximum number of features (words) to keep in the vocabulary\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Fit and transform the text data to TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_train['text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to an array\n",
    "X_tfidf_array = X_tfidf.toarray()\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_length = 100  # Adjust this according to your data\n",
    "X_padded = pad_sequences(X_tfidf_array, maxlen=max_sequence_length)\n",
    "\n",
    "# X_padded now contains your TF-IDF features with padded sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50005039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f5ba016",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, df_train['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87abc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # Dimension of word embeddings\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8225c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0cdda7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1117377 (4.26 MB)\n",
      "Trainable params: 1117377 (4.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1193bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 8s 136ms/step - loss: 0.6941 - accuracy: 0.5097 - val_loss: 0.6936 - val_accuracy: 0.4630\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 5s 126ms/step - loss: 0.6924 - accuracy: 0.5150 - val_loss: 0.6905 - val_accuracy: 0.5370\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.6928 - accuracy: 0.5211 - val_loss: 0.6913 - val_accuracy: 0.5370\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 5s 127ms/step - loss: 0.6929 - accuracy: 0.5198 - val_loss: 0.6911 - val_accuracy: 0.5370\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 5s 129ms/step - loss: 0.6927 - accuracy: 0.5211 - val_loss: 0.6906 - val_accuracy: 0.5370\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 5s 126ms/step - loss: 0.6925 - accuracy: 0.5211 - val_loss: 0.6911 - val_accuracy: 0.5370\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 5s 126ms/step - loss: 0.6926 - accuracy: 0.5211 - val_loss: 0.6914 - val_accuracy: 0.5370\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 4s 122ms/step - loss: 0.6926 - accuracy: 0.5211 - val_loss: 0.6906 - val_accuracy: 0.5370\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 5s 125ms/step - loss: 0.6925 - accuracy: 0.5211 - val_loss: 0.6913 - val_accuracy: 0.5370\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 5s 143ms/step - loss: 0.6924 - accuracy: 0.5211 - val_loss: 0.6908 - val_accuracy: 0.5370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a09f075360>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10  # Number of training epochs\n",
    "batch_size = 64  # Batch size\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9eeeeaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 16ms/step - loss: 0.6908 - accuracy: 0.5370\n",
      "Test Loss: 0.6908356547355652\n",
      "Test Accuracy: 0.5369718074798584\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "089d5232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 196ms/step\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your trained LSTM model and 'tokenizer' is your Tokenizer\n",
    "\n",
    "# Define a function to preprocess a single input text\n",
    "def preprocess_input(text):\n",
    "    # Tokenize the input text\n",
    "    text_sequence = tokenizer.texts_to_sequences([text])\n",
    "    # Pad the sequence\n",
    "    text_sequence_padded = pad_sequences(text_sequence, maxlen=max_sequence_length)\n",
    "    return text_sequence_padded\n",
    "\n",
    "# Define a function to predict sentiment for a single input text\n",
    "def predict_sentiment(input_text):\n",
    "    # Preprocess the input text\n",
    "    input_text_preprocessed = preprocess_input(input_text)\n",
    "    # Make predictions\n",
    "    predictions = model.predict(input_text_preprocessed)\n",
    "    # Assuming 0 indicates negative sentiment and 1 indicates positive sentiment\n",
    "    sentiment = \"Positive\" if predictions[0] > 0.5 else \"Negative\"\n",
    "    return sentiment\n",
    "\n",
    "# Example usage:\n",
    "input_text = \"I want to die\"\n",
    "sentiment = predict_sentiment(input_text)\n",
    "print(\"Sentiment:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0a5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
